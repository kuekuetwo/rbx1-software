{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generating Meta Description Tags using TextSummBert by WordLift",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuekuetwo/rbx1-software/blob/master/Generating_Meta_Description_Tags_using_TextSummBert_by_WordLift.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buZ-c4fi8gcC"
      },
      "source": [
        "# Generating Meta Description Tags\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "  <a href=\"https://wordlift.io\">\n",
        "    <img width=130px src=\"https://wordlift.io/wp-content/uploads/2018/07/logo-assets-510x287.png\" />\n",
        "    </a>\n",
        "    </td>\n",
        "    <td>\n",
        "      by \n",
        "      <a href=\"https://wordlift.io/blog/en/entity/andrea-volpini\">\n",
        "        Andrea Volpini\n",
        "      </a>\n",
        "      <br/>\n",
        "      <br/>\n",
        "      MIT License\n",
        "      <br/>\n",
        "      <br/>\n",
        "      <i>Last updated: <b>December 3rd, 2019</b></i>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eTmScmc9YgQ"
      },
      "source": [
        "You can read the blog post here: https://wordlift.io/blog/en/write-meta-descriptions-bert/\n",
        "\n",
        "## Importing and installing the libraries we need\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OdlLmidEJ7T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a5f01dbb-2a2f-4b3b-cf86-e52229ca446f"
      },
      "source": [
        "!pip install -U git+https://github.com/adbar/trafilatura.git\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "!pip install spacy==2.1.3\n",
        "!pip install transformers\n",
        "!pip install bert-extractive-summarizer==0.2.*\n",
        "\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import csv\n",
        "import os\n",
        "import requests, sys\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import trafilatura\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/adbar/trafilatura.git\n",
            "  Cloning https://github.com/adbar/trafilatura.git to /tmp/pip-req-build-2s1hy8_t\n",
            "  Running command git clone -q https://github.com/adbar/trafilatura.git /tmp/pip-req-build-2s1hy8_t\n",
            "Collecting htmldate>=0.6.2\n",
            "  Downloading https://files.pythonhosted.org/packages/26/17/abb8e6ceedec5bd1a52c3e61acd29bf7eb98e3dea98834ed07bd44244650/htmldate-0.6.2-py3-none-any.whl\n",
            "Collecting justext>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/5f/c7b909b4b864ebcacfac23ce2f6f01a50c53628787cc14b3c06f79464cab/jusText-2.2.0-py2.py3-none-any.whl (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 24.8MB/s \n",
            "\u001b[?25hCollecting readability-lxml>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/af/a7/8ea52b2d3de4a95c3ed8255077618435546386e35af8969744c0fa82d0d6/readability-lxml-0.7.1.tar.gz\n",
            "Collecting lxml>=4.4.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/ba/a0e6866057fc0bbd17192925c1d63a3b85cf522965de9bc02364d08e5b84/lxml-4.5.0-cp36-cp36m-manylinux1_x86_64.whl (5.8MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8MB 52.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests>=2.22.0 in /usr/local/lib/python3.6/dist-packages (from trafilatura==0.4.1) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.8.1 in /usr/local/lib/python3.6/dist-packages (from htmldate>=0.6.2->trafilatura==0.4.1) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: chardet in /usr/local/lib/python3.6/dist-packages (from readability-lxml>=0.7.1->trafilatura==0.4.1) (3.0.4)\n",
            "Collecting cssselect\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->trafilatura==0.4.1) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->trafilatura==0.4.1) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->trafilatura==0.4.1) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.8.1->htmldate>=0.6.2->trafilatura==0.4.1) (1.12.0)\n",
            "Building wheels for collected packages: trafilatura, readability-lxml\n",
            "  Building wheel for trafilatura (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for trafilatura: filename=trafilatura-0.4.1-cp36-none-any.whl size=151365 sha256=f9cedca36703fbecf9e81f9ce20b785086f4c292825cbf592ec0b8f7979ff887\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-g5e9x9t9/wheels/78/ed/6b/4e1987f9c618c11c418e0d7a59ec08310dd900c3c86cd79ceb\n",
            "  Building wheel for readability-lxml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for readability-lxml: filename=readability_lxml-0.7.1-cp36-none-any.whl size=16480 sha256=03459c9e930828e60b62a4302a3af62b7d31f22b2756ded3acb6c971007d30b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/94/48/e5/d944e616d8b0734c3b9cf30a21f4afcf855a1e2b85f82f34fb\n",
            "Successfully built trafilatura readability-lxml\n",
            "Installing collected packages: lxml, htmldate, justext, cssselect, readability-lxml, trafilatura\n",
            "  Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed cssselect-1.1.0 htmldate-0.6.2 justext-2.2.0 lxml-4.5.0 readability-lxml-0.7.1 trafilatura-0.4.1\n",
            "TensorFlow 1.x selected.\n",
            "Collecting spacy==2.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/da/3a1c54694c2d2f40df82f38a19ae14c6eb24a5a1a0dae87205ebea7a84d8/spacy-2.1.3-cp36-cp36m-manylinux1_x86_64.whl (27.7MB)\n",
            "\u001b[K     |████████████████████████████████| 27.7MB 148kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (1.18.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (2.0.3)\n",
            "Collecting preshed<2.1.0,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.4MB/s \n",
            "\u001b[?25hCollecting plac<1.0.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 39.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (0.6.0)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (2.6.0)\n",
            "Collecting thinc<7.1.0,>=7.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/a5/9ace20422e7bb1bdcad31832ea85c52a09900cd4a7ce711246bfb92206ba/thinc-7.0.8-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 64.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3) (3.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.3) (4.38.0)\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: preshed, plac, blis, thinc, spacy\n",
            "  Found existing installation: preshed 3.0.2\n",
            "    Uninstalling preshed-3.0.2:\n",
            "      Successfully uninstalled preshed-3.0.2\n",
            "  Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed blis-0.2.4 plac-0.9.6 preshed-2.0.1 spacy-2.1.3 thinc-7.0.8\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 9.4MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 32.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 41.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.13.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 60.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.16.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.1->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.1->boto3->transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=9ae1336b9f2b4988c43e38f98b420d8941208f985a1cbde609c67388ab4e3e7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.8.0\n",
            "Collecting bert-extractive-summarizer==0.2.*\n",
            "  Downloading https://files.pythonhosted.org/packages/cb/ee/2db102975d9ce3a2c0e975c0f2a93a19f97e8891bbaa17232d49d21f7670/bert-extractive-summarizer-0.2.2.tar.gz\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (from bert-extractive-summarizer==0.2.*) (2.8.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from bert-extractive-summarizer==0.2.*) (0.22.2.post1)\n",
            "Collecting neuralcoref\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/24/0ec7845a5b73b637aa691ff4d1b9b48f3a0f3369f4002a59ffd7a7462fdb/neuralcoref-4.0-cp36-cp36m-manylinux1_x86_64.whl (287kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy==2.1.3 in /usr/local/lib/python3.6/dist-packages (from bert-extractive-summarizer==0.2.*) (2.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer==0.2.*) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer==0.2.*) (1.18.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer==0.2.*) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer==0.2.*) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer==0.2.*) (1.13.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer==0.2.*) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer==0.2.*) (4.38.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer==0.2.*) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer==0.2.*) (0.1.86)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer==0.2.*) (2019.12.20)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->bert-extractive-summarizer==0.2.*) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->bert-extractive-summarizer==0.2.*) (1.4.1)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->bert-extractive-summarizer==0.2.*) (2.0.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->bert-extractive-summarizer==0.2.*) (1.0.2)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->bert-extractive-summarizer==0.2.*) (2.6.0)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->bert-extractive-summarizer==0.2.*) (0.9.6)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->bert-extractive-summarizer==0.2.*) (0.2.4)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->bert-extractive-summarizer==0.2.*) (7.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->bert-extractive-summarizer==0.2.*) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->bert-extractive-summarizer==0.2.*) (2.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3->bert-extractive-summarizer==0.2.*) (0.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->bert-extractive-summarizer==0.2.*) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->bert-extractive-summarizer==0.2.*) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->bert-extractive-summarizer==0.2.*) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->bert-extractive-summarizer==0.2.*) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->bert-extractive-summarizer==0.2.*) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->bert-extractive-summarizer==0.2.*) (1.12.0)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->bert-extractive-summarizer==0.2.*) (1.16.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->bert-extractive-summarizer==0.2.*) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->bert-extractive-summarizer==0.2.*) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.1->boto3->transformers->bert-extractive-summarizer==0.2.*) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.1->boto3->transformers->bert-extractive-summarizer==0.2.*) (2.8.1)\n",
            "Building wheels for collected packages: bert-extractive-summarizer\n",
            "  Building wheel for bert-extractive-summarizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-extractive-summarizer: filename=bert_extractive_summarizer-0.2.2-cp36-none-any.whl size=7818 sha256=a5cef2a8f4ed3800f83bbfc0d12e6483e382cbddfa8bcc7683a66c8ef53b904c\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/6c/80/16f29b3065ee431e1e8248f8e3e73ced6d195c1df6301672fa\n",
            "Successfully built bert-extractive-summarizer\n",
            "Installing collected packages: neuralcoref, bert-extractive-summarizer\n",
            "Successfully installed bert-extractive-summarizer-0.2.2 neuralcoref-4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t41cZ9efGGo5"
      },
      "source": [
        "## Downloading crawl data from Google Sheet \n",
        "\n",
        "The script uses the _url` CSV file generated with **WooRank Crawler** (or alternatively the data from **Screaming Frog**) that provides the list of URLs and the information of where the MD is missing.  \n",
        "\n",
        "The data has been imported into Google Sheet so that we can inspect it. Change the URL below after publishing your CSV:\n",
        "\n",
        "\n",
        "> 1. Open file from \"My Drive\" or \"Upload\"\n",
        "2. File -> Publish to the web -> \"Sheet name\" option and \"csv\" option\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjseHMCmsp8C"
      },
      "source": [
        "### Using WooRank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag1C6HlcHszA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "6a9d96b5-1ee7-4507-c127-13d694912394"
      },
      "source": [
        "# Download the list of URLs from Google Docs (file generated with WooRank) \n",
        "# Replace the following with a crawl from your favorite website that you have published on Google Drive\n",
        "!wget 'https://docs.google.com/spreadsheets/d/e/2PACX-1vRKcg1Ly4wD2ANquGnZCgUZv22lVPcRvMlTyzhLSavnH97VSPGhm0qC7U2ggVl330aFauJOftTxGIhQ/pub?gid=217899676&single=true&output=csv'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-07 16:22:55--  https://docs.google.com/spreadsheets/d/e/2PACX-1vRKcg1Ly4wD2ANquGnZCgUZv22lVPcRvMlTyzhLSavnH97VSPGhm0qC7U2ggVl330aFauJOftTxGIhQ/pub?gid=217899676&single=true&output=csv\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.20.138, 74.125.20.139, 74.125.20.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.20.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘pub?gid=217899676&single=true&output=csv.2’\n",
            "\n",
            "\r          pub?gid=2     [<=>                 ]       0  --.-KB/s               \rpub?gid=217899676&s     [ <=>                ] 731.05K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-05-07 16:22:55 (6.67 MB/s) - ‘pub?gid=217899676&single=true&output=csv.2’ saved [748591]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsF8H4sdIIfI"
      },
      "source": [
        "#### Creating a Pandas DataFrame from WooRank data\n",
        "\n",
        "\n",
        "Following the file structure generated using the WooRank's crawler, we will use the following columns:\n",
        "\n",
        "- *url* (`cols='0'` | `url`), \n",
        "- *status code* (`cols='5'` | `status`),\n",
        "- *page type* (`cols='8'` | `parent_type`)\n",
        "- *internal or esternal* (`cols='12'` | `from_internal`)\n",
        "- *position* (`cols='38'` | `position`)\n",
        "- *meta description lenght in px* (`cols='46'` | `description_len_px`)\n",
        "\n",
        "We will then use *http status* to focus our analysis only to urls responding with `HTTP 200`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ntyfjL3H7-b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "28e04843-87e5-427e-ce70-e50c6b5e3095"
      },
      "source": [
        "df = pd.read_csv('pub?gid=217899676&single=true&output=csv.2', # Update the string here to change the file\n",
        "                 usecols=[0,5,8,12,38,46],  \n",
        "                 header=0,\n",
        "                 encoding=\"utf-8-sig\" ) \n",
        "\n",
        "print(\"we have a total of:\", len(df), \" urls\")\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "we have a total of: 1707  urls\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>status</th>\n",
              "      <th>parent_type</th>\n",
              "      <th>from_internal</th>\n",
              "      <th>position</th>\n",
              "      <th>description_len_px</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://wordlift.io/robots.txt</td>\n",
              "      <td>302</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://wordlift.io/robots.txt</td>\n",
              "      <td>200</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://wordlift.io/blog/en/entity/semantic-seo</td>\n",
              "      <td>301</td>\n",
              "      <td>PAGE</td>\n",
              "      <td>yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://wordpress.org/plugins/wordlift/</td>\n",
              "      <td>200</td>\n",
              "      <td>PAGE</td>\n",
              "      <td>no</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://vimeo.com/io10</td>\n",
              "      <td>200</td>\n",
              "      <td>PAGE</td>\n",
              "      <td>no</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               url  ...  description_len_px\n",
              "0                    http://wordlift.io/robots.txt  ...                 NaN\n",
              "1                   https://wordlift.io/robots.txt  ...                 NaN\n",
              "2  https://wordlift.io/blog/en/entity/semantic-seo  ...                 NaN\n",
              "3          https://wordpress.org/plugins/wordlift/  ...                 NaN\n",
              "4                           https://vimeo.com/io10  ...                 NaN\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atX3kesIyEBP"
      },
      "source": [
        "#### Finding all URLs where meta description are missing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCxyE4V1xvEs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "2ead1c67-a7ae-4f9b-e815-212bbf878efc"
      },
      "source": [
        "# Keep all rows representing a page with status = 200, with md either null or 0, from the English blog and with Position < 15 \n",
        " \n",
        "df = df[(df['from_internal'] != 'no') & (df['status'] == 200) & (df['parent_type'] == 'PAGE') & ((df['description_len_px'].isnull()) | (df['description_len_px']== 0)) & (df['url'].str.contains(\"blog/it\")) & (df['position'] < 15) & (df['position'] > 3)] # Use this with WooRank\n",
        "\n",
        "print(\"we have to process:\", len(df), \" urls\")\n",
        "\n",
        "# Reindex df\n",
        "df.index = range(len(df.index))\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "we have to process: 22  urls\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>status</th>\n",
              "      <th>parent_type</th>\n",
              "      <th>from_internal</th>\n",
              "      <th>position</th>\n",
              "      <th>description_len_px</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://wordlift.io/blog/it/vocabolario/wordca...</td>\n",
              "      <td>200</td>\n",
              "      <td>PAGE</td>\n",
              "      <td>yes</td>\n",
              "      <td>10.755906</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://wordlift.io/blog/it/vocabolario/json-ld/</td>\n",
              "      <td>200</td>\n",
              "      <td>PAGE</td>\n",
              "      <td>yes</td>\n",
              "      <td>11.821712</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://wordlift.io/blog/it/vocabolario/thubte...</td>\n",
              "      <td>200</td>\n",
              "      <td>PAGE</td>\n",
              "      <td>yes</td>\n",
              "      <td>6.739726</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://wordlift.io/blog/it/vocabolario/robert...</td>\n",
              "      <td>200</td>\n",
              "      <td>PAGE</td>\n",
              "      <td>yes</td>\n",
              "      <td>11.796475</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://wordlift.io/blog/it/vocabolario/wordlift/</td>\n",
              "      <td>200</td>\n",
              "      <td>PAGE</td>\n",
              "      <td>yes</td>\n",
              "      <td>10.535714</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 url  ...  description_len_px\n",
              "0  https://wordlift.io/blog/it/vocabolario/wordca...  ...                 0.0\n",
              "1   https://wordlift.io/blog/it/vocabolario/json-ld/  ...                 0.0\n",
              "2  https://wordlift.io/blog/it/vocabolario/thubte...  ...                 0.0\n",
              "3  https://wordlift.io/blog/it/vocabolario/robert...  ...                 0.0\n",
              "4  https://wordlift.io/blog/it/vocabolario/wordlift/  ...                 0.0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBcyTfZ4tVJ8"
      },
      "source": [
        "### Using Screaming Frog"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JALL291GtZap",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "39e1754c-4fd3-47d2-8872-8dd9792e77ec"
      },
      "source": [
        "# Download the list of URLs from Google Docs (file generated with Screaming Frog SEO Spider) \n",
        "# Replace the following with a crawl from your favorite website that you have published on Google Drive\n",
        "\n",
        "!wget 'https://docs.google.com/spreadsheets/d/e/2PACX-1vTGpl7KboITzqC8d-rosX_H4geyib-kHrVtVwrhM9rZSie7X35vYvC8iVJLVwGOYTemC4xm1qduMU8v/pub?gid=662239818&single=true&output=csv'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-04 09:00:03--  https://docs.google.com/spreadsheets/d/e/2PACX-1vTGpl7KboITzqC8d-rosX_H4geyib-kHrVtVwrhM9rZSie7X35vYvC8iVJLVwGOYTemC4xm1qduMU8v/pub?gid=662239818&single=true&output=csv\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.142.139, 74.125.142.138, 74.125.142.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.142.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘pub?gid=662239818&single=true&output=csv’\n",
            "\n",
            "\r          pub?gid=6     [<=>                 ]       0  --.-KB/s               \rpub?gid=662239818&s     [ <=>                ]  97.97K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-12-04 09:00:03 (2.52 MB/s) - ‘pub?gid=662239818&single=true&output=csv’ saved [100324]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb2AppotwUQd"
      },
      "source": [
        "#### Creating a Pandas DataFrame from Screaming Frog data\n",
        "\n",
        "\n",
        "Following the file structure generated using the Screaming Frog's crawler, we will use the following columns:\n",
        "\n",
        "- *url* (`cols='0'` | `Address`), \n",
        "- *http status* (`cols='2'` | `Status Code`), \n",
        "- *meta description lenght* (`cols='11'` | `Meta Description 1 Length`),\n",
        "- *position* (`cols='48'` | `Position`),\n",
        "\n",
        "We will then use *http status* to focus our analysis only to urls responding with `HTTP 200`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyPnprLnxXCP"
      },
      "source": [
        "df = pd.read_csv('pub?gid=662239818&single=true&output=csv', # Update the string here to change the file\n",
        "                 usecols=[0,2,11,48],  \n",
        "                 header=0,\n",
        "                 encoding=\"utf-8-sig\" ) \n",
        "\n",
        "print(\"we have a total of:\", len(df), \" urls\")\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqnuyDjZyKfT"
      },
      "source": [
        "#### Finding all URLs where meta description are missing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKn4dAAGyPB2"
      },
      "source": [
        "# Keep all rows representing a page with status = 200, with md 0, from the Italian blog and with Position < 15 \n",
        " \n",
        "df = df[(df['Status Code'] == 200) & ((df['Meta Description 1 Pixel Width']== 0)) & (df['Address'].str.contains(\"blog/it\")) & (df['Position'] < 15) & (df['Position'] > 3)] # Use this with Screaming Frog\n",
        "\n",
        "print(\"we have to process:\", len(df), \" urls\")\n",
        "\n",
        "# Reindex df\n",
        "df.index = range(len(df.index))\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ2rVSKy2MWB"
      },
      "source": [
        "## Summarizing \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcD42IUDqlYb"
      },
      "source": [
        "## Running the analysis \n",
        "\n",
        "In the next cells we have one function called `url_to_string` to get the text from a URL (make sure to fine-tune this one if you know the class that contains the body of the article on your website) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-NV-LUQqy1u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "a8dcd870-38d6-498b-9bb0-dc29d99a8972"
      },
      "source": [
        "# Get clean text from URL\n",
        "\n",
        "def url_to_string(url):\n",
        "  try:\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0'}\n",
        "    res = requests.get(url, headers=headers)\n",
        "    html = res.text\n",
        "    soup = BeautifulSoup(html, 'html5lib')\n",
        "    for script in soup([\"script\", \"style\", 'aside']):\n",
        "        script.extract()\n",
        "    \n",
        "    # uncomment the lines in the if/else block and comment the one after if you know the name of the class containing the article body \n",
        "    if isinstance(soup.find('div', {'class' :'entry-content'}), type(None)): # here is the div containing the content\n",
        "      return \" \".join(re.split(r'[\\n\\t]+', soup.get_text()))\n",
        "    else:\n",
        "      return \" \".join(re.split(r'[\\n\\t]+', soup.find('div', {'class' :'entry-content'}).text))   \n",
        "\n",
        "  except requests.exceptions.HTTPError as err:\n",
        "    print(err)\n",
        "    sys.exit(1)\n",
        "    return err\n",
        "\n",
        "'''\n",
        "# Get clean text from URL using Trafilatura\n",
        "\n",
        "def url_to_string(url):\n",
        "  try:\n",
        "    downloaded = trafilatura.fetch_url(url)\n",
        "    if downloaded is not None: # assuming the download was successful\n",
        "      result = trafilatura.extract(downloaded, include_tables=False, include_formatting=False, include_comments=False) \n",
        "    return result\n",
        "  except ValueError as err:\n",
        "    print(err)\n",
        "    sys.exit(1)\n",
        "    return err\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-8140a3196bbb>\"\u001b[0;36m, line \u001b[0;32m34\u001b[0m\n\u001b[0;31m    return err\u001b[0m\n\u001b[0m              \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hp_UN8x14nw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883
        },
        "outputId": "ea33b8f9-9846-4365-8f09-e0f22b196514"
      },
      "source": [
        "# Create a list to store the MDs\n",
        "data_x = [] \n",
        "\n",
        "from summarizer import Summarizer\n",
        "# For each URL in the input CSV run the analysis and store the results in the list \n",
        "for i in range(len(df)):\n",
        "    # Here is the URL to be analyzed\n",
        "    line = df.iloc[i][0]\n",
        "\n",
        "\t# Error handling for HTTP connection problems\n",
        "    try:\n",
        "       body = url_to_string(line)\n",
        "    except:\n",
        "    \tprint('error while fetching', line, err)\n",
        "    \n",
        "\t# BERT\n",
        "    print(\"Summarizing URL via BERT: \" + line)\n",
        "    model = Summarizer()\n",
        "    result = model(body, min_length=60, ratio=0.005)\n",
        "    full = ''.join(result)\n",
        "    print(full)\n",
        "\n",
        "\t# Storing all values into the list \n",
        "    data_x.append({\"url\":line, \"BERT\":full})\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summarizing URL via BERT: https://wordlift.io/blog/it/vocabolario/wordcamp-europe-2019/\n",
            "Dal 20 al 22 giugno, la comunità di WordPress si è riunita a Berlino in occasione del WordCamp Europe (#WCEU) e, ovviamente, il nostro team non poteva mancare all’appello. Matt è salito sul palco per spiegare come l’editor a blocchi di Gutenberg abbia aggiunto una serie di notevoli miglioramenti, tra cui le funzionalità di gestione dei blocchi, un blocco di copertina con elementi nidificati, widget da integrare come blocchi, raggruppamenti di blocchi e avvisi in stile snackbar.\n",
            "Summarizing URL via BERT: https://wordlift.io/blog/it/vocabolario/json-ld/\n",
            "JSON-LD sta per JavaScript Object Notation per i Linked Data ed è un formato leggero per i Linked Data, per leggere e scrivere in modo semplice i metadati sul web. Queste entità hanno ID unici (unique resource identifier) nel web dei dati e grazie a questi ID, WordLift estrae delle informazioni aggiuntive e le inietta nelle pagine web usando JSON-LD.\n",
            "Summarizing URL via BERT: https://wordlift.io/blog/it/vocabolario/thubten-gyatso/\n",
            "Nel 1878 è stati riconosciuto come la reincarnazione del Dalai Lams. Nel 1879 è stato incoronato nel Potala Palace, ma non ha assunto il potere politico fino al 1985, dopo aver raggiunto la maggiore età.\n",
            "Summarizing URL via BERT: https://wordlift.io/blog/it/vocabolario/roberto-serra/\n",
            "Organizzatore del #WMT2017, ha anche portato un caso studio su una startup editoriale, arrivata in pochi anni a eguagliare le realtà leader del suo mercato. Continua a leggere l’articolo sulle 9 strategie di web marketing di cui si è discusso durante l’ultima edizione del #WMT2018 di Cagliari!\n",
            "Summarizing URL via BERT: https://wordlift.io/blog/it/vocabolario/wordlift/\n",
            "WordLift è una startup innovativa romana che ha la missione di trasformare il web in librerie digitali. Il knowlegde graph è pubblicato sotto forma di linked data che possono essere usati per:\n",
            "- organizzare i contenuti del sito web,\n",
            "- offrire raccomandazioni di contenuti rilevanti ai lettori,\n",
            "- aggiungere metadati comprensibili per le macchine.\n",
            "Summarizing URL via BERT: https://wordlift.io/blog/it/vocabolario/elaborazione-del-linguaggio-naturale/\n",
            "Il Natural language processing (anche detto elaborazione del linguaggio naturale o NLP) è un campo di studi che unisce informatica, intelligenza artificiale e linguistica che ha a che fare con le interazioni tra i computer e gli esseri umani attraverso il linguaggio naturale. Tra le sfide dell’NLP c’è la comprensione del linguaggio naturale – che significa, in altre parole, abilitare i computer a comprendere il significato del linguaggio umano.\n",
            "Summarizing URL via BERT: https://wordlift.io/blog/it/vocabolario/chi-e-luca-conti/\n",
            "Luca Conti è un blogger, consulente di web marketing e scrittore che insegna alla Milano-Bicocca.\n",
            "Summarizing URL via BERT: https://wordlift.io/blog/it/vocabolario/rand-fishkin/\n",
            "Rand Fishkin è il fondatore di Moz, una delle società più influenti nel settore SEO, presenta il Whiteboard Friday, è co-autore di alcuni libri sulla SEO, co-founder di Inbound.org e fa parte del consiglio direttivo del software di presentazione Haiku Deck.\n",
            "Summarizing URL via BERT: https://wordlift.io/blog/it/vocabolario/marina-pitzoi/\n",
            "Social media e community manager, Marina Pitzoi è una consulente e web writer, laureata in Economia e Management del Turismo al Polo Universitario di Olbia.\n",
            "Summarizing URL via BERT: https://wordlift.io/blog/it/vocabolario/alfonso-biondi/\n",
            "Quando non scrive le sue battute caustiche, Afonso lavora come web content specialist per il più importante editore online italiano. Scrive per Lercio.it, considerandola la cosa più bella che gli sia capitata dopo la scoperta dell’autoerotismo.“\n",
            "Summarizing URL via BERT: https://wordlift.io/blog/it/vocabolario/valentina-ferrero/\n",
            "Diario Innovazione, un magazine digitale del gruppo Diario del Web focalizzato sull’innovazione, la tecnologia e l’imprenditoria.\n",
            "Summarizing URL via BERT: https://wordlift.io/blog/it/vocabolario/oscar-valentini/\n",
            "Insegnante di arti marziali e meditazione, è l’ideatore del massaggio TAO. Tao Roma, promuove l’integrazione di corpo e mente per lo sviluppo personale e la ricerca spirituale.\n",
            "Summarizing URL via BERT: https://wordlift.io/blog/it/vocabolario/wordcamp-roma-2017/\n",
            "Un WordCamp è un evento organizzato da e per la Community di WordPress: è l’occasione migliore per incontrare persone che amano questa piattaforma, ampliare la tua rete e imparare nuove cose sul CMS più popolare al mondo. Siamo orgogliosi di far parte della vivace Community romana di WordPress: abbiamo contribuito attivamente alla sua nascita e alla sia crescita, ecco perché siamo davvero felici di poter contribuire al primo WordCamp Roma. ?\n",
            "Summarizing URL via BERT: https://wordlift.io/blog/it/vocabolario/luca-sartoni/\n",
            "Growth Engineer per Automattic, Luca Sartoni è un blogger italiano di successo, spesso impegnato come speaker in eventi nazionali e internazionali.\n",
            "Summarizing URL via BERT: https://wordlift.io/blog/it/vocabolario/filippo-sogus/\n",
            "È stato relatore per il #WMT2017, dove ha presentato un caso studio sulla strategia di ottimizzazione SEO per un e-commerce.\n",
            "Summarizing URL via BERT: https://wordlift.io/blog/it/vocabolario/adam-lynch/\n",
            "Si occupa di digital marketing multipiattaforma ed è appassionato di tecnologie, startup, golf e cucina.\n",
            "Summarizing URL via BERT: https://wordlift.io/blog/it/vocabolario/s-voice/\n",
            "S Voice è un software per il riconoscimento vocale sviluppato da Samsung e presentato con il lancio dello smartphone Galaxy S III il 3 maggio 2012.\n",
            "Summarizing URL via BERT: https://wordlift.io/blog/it/vocabolario/valentina-falcinelli/\n",
            "Fondatrice e direttrice della web agency Pennamontata, Valentina lavora come copywriter da dieci anni: tra gli altri, scrive per Subito, BPER Banca e WWF Italia. Insegna tecniche di copywriting condividendo la sua esperienza con un sorriso.\n",
            "Summarizing URL via BERT: https://wordlift.io/blog/it/vocabolario/william-sbarzaglia/\n",
            "Ha lavorato per Lamborghini, per il Parlamento Europeo, per l’Istat e per molte altre realtà di primo piano.\n",
            "Summarizing URL via BERT: https://wordlift.io/blog/it/vocabolario/larry-kim/\n",
            "Per avviare la società, forniva servizi di consulenza in modo da poter finanziare e gestire un team di ingegneri e marketer per lo sviluppo e la commercializzazione di un software per l’automazione del search marketing. In precedenza ha ottenuto un posto nella Hall of Fame del ClickZ Digital Marketing e ha vinto il premio Small Business Influencer Award.\n",
            "Summarizing URL via BERT: https://wordlift.io/blog/it/vocabolario/michael-vittori/\n",
            "SEO e Facebook Ads, si occupa dal 2008 di web marketing, lavorando con imprenditori e professionisti, soprattutto in ambito turistico e e-commerce.\n",
            "Summarizing URL via BERT: https://wordlift.io/blog/it/vocabolario/veronica-rizzo/\n",
            "SEO, dal 2013 si occupa di campagne di Local Search Marketing per attività locali e catene di negozi e franchising.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p00mk90Szg6w"
      },
      "source": [
        "### Testing BERT Multilingual\n",
        "\n",
        "This cell is alternative to the cells above and will load a varian of BERT called `bert-base-multilingual-cased`.\n",
        "\n",
        "Trained on cased text in the top **104 languages** with the largest Wikipedias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifLNguJP0T8S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "outputId": "518f6b40-d0ce-42bd-b705-fe443e29fe17"
      },
      "source": [
        "# Create a list to store the MDs\n",
        "data_x = [] \n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "bert_model = BertModel.from_pretrained('bert-base-multilingual-cased', output_hidden_states=True)\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "from summarizer import Summarizer\n",
        "# For each URL in the input CSV run the analysis and store the results in the list \n",
        "for i in range(len(df)):\n",
        "    # Here is the URL to be analyzed\n",
        "    line = df.iloc[i][0]\n",
        "\n",
        "\t# Error handling for HTTP connection problems\n",
        "    try:\n",
        "       body = url_to_string(line)\n",
        "    except:\n",
        "    \tprint('error while fetching', line, err)\n",
        "    \n",
        "\t# BERT\n",
        "    print(\"Summarizing URL via BERT  ML: \" + line)\n",
        "    model = Summarizer(custom_model=bert_model, custom_tokenizer=bert_tokenizer)\n",
        "    result = model(body, min_length=60, ratio=0.005)\n",
        "    full = ''.join(result)\n",
        "    print(full)\n",
        "\n",
        "\t# Storing all values into the list \n",
        "    data_x.append({\"url\":line, \"BERT\":full})\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summarizing URL via BERT  ML: https://wordlift.io/blog/it/vocabolario/wordcamp-europe-2019/\n",
            "Dal 20 al 22 giugno, la comunità di WordPress si è riunita a Berlino in occasione del WordCamp Europe (#WCEU) e, ovviamente, il nostro team non poteva mancare all’appello. Matteoc and Cyberandy sul palco del WCEU\n",
            "Google sta aumentando il suo impegno nell’ecosistema WordPress e per questa edizione di WCEU è stato introdotto un nuovo strumento chiamato Google Site Kit.\n",
            "Summarizing URL via BERT  ML: https://wordlift.io/blog/it/vocabolario/json-ld/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-3fa8df472340>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Summarizing URL via BERT  ML: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mfull\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/summarizer/model_processors.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, body, ratio, min_length, max_length, use_first, algorithm)\u001b[0m\n\u001b[1;32m     58\u001b[0m     def __call__(self, body: str, ratio: float=0.2, min_length: int=40, max_length: int=600,\n\u001b[1;32m     59\u001b[0m                  use_first: bool=True, algorithm='kmeans') -> str:\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/summarizer/model_processors.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, body, ratio, min_length, max_length, use_first, algorithm)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kmeans'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     ) -> str:\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_content_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/summarizer/model_processors.py\u001b[0m in \u001b[0;36mprocess_content_sentences\u001b[0;34m(self, body, min_length, max_length)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess_content_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoref_resolved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;31m#call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \"\"\"\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m             raise ValueError(\n\u001b[1;32m    375\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNq3kDN3WRQt"
      },
      "source": [
        "### Testing the brand new ALBERT implementation\n",
        "\n",
        "This cell is alternative to the cell above and will load ALBERT (see: \"[ALBERT: A Lite BERT For Self-Supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)\") "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7THygT5WPAL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        },
        "outputId": "c17d9bbd-a84e-48d5-fd55-a7fe41d6c1a3"
      },
      "source": [
        "# Create a list to store the MDs\n",
        "data_x = [] \n",
        "\n",
        "from transformers import AlbertTokenizer, AlbertModel\n",
        "\n",
        "albert_model = AlbertModel.from_pretrained('albert-base-v1', output_hidden_states=True)\n",
        "albert_tokenizer = AlbertTokenizer.from_pretrained('albert-base-v1')\n",
        "\n",
        "from summarizer import Summarizer\n",
        "# For each URL in the input CSV run the analysis and store the results in the list \n",
        "for i in range(len(df)):\n",
        "    # Here is the URL to be analyzed\n",
        "    line = df.iloc[i][0]\n",
        "\n",
        "\t# Error handling for HTTP connection problems\n",
        "    try:\n",
        "       body = url_to_string(line)\n",
        "    except:\n",
        "    \tprint('error while fetching', line, err)\n",
        "    \n",
        "\t# BERT\n",
        "    print(\"Summarizing URL via ALBERT: \" + line)\n",
        "    model = Summarizer(custom_model=albert_model, custom_tokenizer=albert_tokenizer)\n",
        "    result = model(body, min_length=60, ratio=0.005)\n",
        "    full = ''.join(result)\n",
        "    print(full)\n",
        "\n",
        "\t# Storing all values into the list \n",
        "    data_x.append({\"url\":line, \"BERT\":full})\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summarizing URL via ALBERT: https://wordlift.io/blog/en/entity/freeyork/\n",
            "%More sessions from GoogleFounded by Sam Isma in 2009, Freeyork is a community-driven design magazine which aims to spread the works and stories of upcoming artists. ”Sam Isma, Founder of FreeyorkThe ResultsAfter the fist three months, WordLift improved the number of organic sessions (+18.47% increase of sessions from Google) and the number of new users with a double digit growth (+12.13% of new users).On average, pages enriched with WordLift compared with all the other pages, are performing 2.4 times better in terms of page views and in terms of sessions.\n",
            "Summarizing URL via ALBERT: https://wordlift.io/blog/en/entity/fact-checking/\n",
            "According to Wikipedia, fact checking is:“Fact checking is the act of checking factual assertions in non-fictional text in order to determine the veracity and correctness of the factual statements in the text.\n",
            "Summarizing URL via ALBERT: https://wordlift.io/blog/en/entity/wordpress/\n",
            "WordPress is a free and open source blogging tool and content management system (CMS) based on PHP and MySQL. WordPress is currently the most popular blogging system in use on the Web, powering over 8 Content Management System and the trend is growingInsideout10 is the company that created WordLift the semantic editor for WordPress, the online video platform HelixWare, and has started the WordPress Meetup in Rome.\n",
            "Summarizing URL via ALBERT: https://wordlift.io/blog/en/entity/teodora-petkova/\n",
            "Teodora Petkova is a philologist and a freelance content writer with an educational background is in Classical Studies and Creative Writing. She has a past as a Latin Teacher at the University of Sofia, and she still loves to share her knowledge, especially about the web writing in the context of the semantic web.\n",
            "Summarizing URL via ALBERT: https://wordlift.io/blog/en/entity/html/\n",
            "HyperText Markup Language (HTML) is the main markup language for creating web pages and other information that can be displayed in a web browser. HTML is written in the form of HTML elements consisting of tags enclosed in angle brackets (like), within the web page content.\n",
            "Summarizing URL via ALBERT: https://wordlift.io/blog/en/entity/travel/\n",
            "The hospitality industry is a broad category of fields within the service industry that includes lodging, event planning, theme parks, transportation, cruise line, traveling and additional fields within the tourism industry.\n",
            "Summarizing URL via ALBERT: https://wordlift.io/blog/en/entity/uniform-resource-identifier/\n",
            "In computing, a uniform resource identifier (URI) is a string of characters used to identify a name or a web resource. What is a URN then?Uniform Resource Name (URN) is the historical name for a Uniform Resource Identifier (URI) that uses the urn scheme.\n",
            "Summarizing URL via ALBERT: https://wordlift.io/blog/en/entity/iks-research-projects/\n",
            "IKS (Interactive Knowledge Stack) is an open source community, whose research projects have being financed by the EU and focused on building an open and flexible technology platform to semantically enhance any Content Management System (CMS).WordLift is one of the results of this research, applying semantic web to WordPress.\n",
            "Summarizing URL via ALBERT: https://wordlift.io/blog/en/entity/gennaro-cuofano/\n",
            "Gennaro is a digital entrepreneur specialized in growing online businesses. As Head of Business Development for WordLift, Gennaro focused on finding the right strategies, distributions, sales processes to speed up the growth of the business.\n",
            "Summarizing URL via ALBERT: https://wordlift.io/blog/en/entity/wordlift/\n",
            "WordLift is an innovative startup based in Rome whose mission is to transform the Web into digital libraries.\n",
            "Summarizing URL via ALBERT: https://wordlift.io/blog/en/entity/britney-muller/\n",
            "Britney Muller is part of the marketing team at MOZ, where she is focused on SEO and content architecture.\n",
            "Summarizing URL via ALBERT: https://wordlift.io/blog/en/entity/rainer-edlinger/\n",
            "Rainer is an Austrian blogger and online marketing manager at SalzburgerLand Tourismus GmbH, where he works together with Martin Reichhart.\n",
            "Summarizing URL via ALBERT: https://wordlift.io/blog/en/entity/google-launches-knowledge-graph-2012/\n",
            "It is one of the most important changes in the SERP of Google since the very beginning. In May 2012 users start to see large panels with additional factual information about the topic you were searching over the right side of Google’s search result pages.\n",
            "Summarizing URL via ALBERT: https://wordlift.io/blog/en/entity/luca-conti/\n",
            "Luca Conti is an Italian blogger, marketing consultant, author of many books, and professor at Milano-Bicocca. In 2002 Luca created his personal blog, Pandemia.info whose tagline claims “learning something new every single day” and keeps the promise giving constantly to its readers some new meaningful point-of-view on technology, media, marketing, and lifestyle.\n",
            "Summarizing URL via ALBERT: https://wordlift.io/blog/en/entity/technology/\n",
            "The word technology refers to the making, modification, usage, and knowledge of tools, machines, techniques, crafts, systems, and methods of organization, in order to solve a problem, improve a preexisting solution to a problem, achieve a goal, handle an applied input/output relation or perform a specific function.\n",
            "Summarizing URL via ALBERT: https://wordlift.io/blog/en/entity/the-fox-and-the-crow/\n",
            "Wikipedia says:The Fox and the Crow is one of Aesop’s Fables, numbered 124 in the Perry Index.\n",
            "Summarizing URL via ALBERT: https://wordlift.io/blog/en/entity/roberto-serra/\n",
            "Roberto Serra is an Italian Digital marketing consultant specialized in the digital launch of new businesses. He organizes the Web Marketing Training, the biggest web-marketing event in Sardinia, which hosts national and international speakers.\n",
            "Summarizing URL via ALBERT: https://wordlift.io/blog/en/entity/valentina-ferrero/\n",
            "Editor in Chief for Diario Innovazione, a digital magazine of the group Diario del Web focused on innovation, technology and young enterpreneurship. Valentina Ferrero writes about innovation, looking closely at the perpective of Italian enterpreneurs through interviews and storytelling.\n",
            "Summarizing URL via ALBERT: https://wordlift.io/blog/en/entity/alfonso-biondi/\n",
            "Satirical writer for Lercio.it, Il Vernacoliere and Acido Lattico. When he is not writing his caustic jokes, Afonso works as web content specialist for the biggest Italian Web Publisher.\n",
            "Summarizing URL via ALBERT: https://wordlift.io/blog/en/entity/elena-pavoncello/\n",
            "Elena started her career in Saatchi when it wasn’t named Saatchi, yet, and copywriters had to write with a pen and a piece of paper.\n",
            "Summarizing URL via ALBERT: https://wordlift.io/blog/en/entity/cosebelle-magazine/\n",
            "Cosebelle Magazine is an Italian web-magazine founded on November 2010 with the intent of showing all the beautiful things that our world has to offer. The all-women editorial staff daily deals with the most different topics from music and traveling to architecture and recipes, all their entries are gracefully curated.\n",
            "Summarizing URL via ALBERT: https://wordlift.io/blog/en/entity/intertwingularity/\n",
            "The word intertwingularity was coined by internet pioneer Ted Nelson to represent the idea that everything in the universe is deeply interconnected. Ted Nelson, Lost in hyperspaceThe Promise of IntertwingularityEverything, from code to culture, is connected and the potential of a linking mechanism that can map this connectedness is as fascinating as it is hard to implement.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZSEg-Bx61ea"
      },
      "source": [
        "## Storing data \n",
        "\n",
        "In the following cells we are going to save a CSV containing for each url the summaries generated by the different algos. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-46i_W868LU"
      },
      "source": [
        "# Save results to the output CSV\n",
        "df_new = pd.DataFrame(data_x, columns=[\"url\", \"BERT\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3eT90mn7oac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e06c1996-56c9-4217-ccd8-ebe08d473be7"
      },
      "source": [
        "df_new.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>BERT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://wordlift.io/blog/it/vocabolario/wordca...</td>\n",
              "      <td>Dal 20 al 22 giugno, la comunità di WordPress ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://wordlift.io/blog/it/vocabolario/json-ld/</td>\n",
              "      <td>JSON-LD sta per JavaScript Object Notation per...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://wordlift.io/blog/it/vocabolario/thubte...</td>\n",
              "      <td>Nel 1878 è stati riconosciuto come la reincarn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://wordlift.io/blog/it/vocabolario/robert...</td>\n",
              "      <td>Organizzatore del #WMT2017, ha anche portato u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://wordlift.io/blog/it/vocabolario/wordlift/</td>\n",
              "      <td>WordLift è una startup innovativa romana che h...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 url                                               BERT\n",
              "0  https://wordlift.io/blog/it/vocabolario/wordca...  Dal 20 al 22 giugno, la comunità di WordPress ...\n",
              "1   https://wordlift.io/blog/it/vocabolario/json-ld/  JSON-LD sta per JavaScript Object Notation per...\n",
              "2  https://wordlift.io/blog/it/vocabolario/thubte...  Nel 1878 è stati riconosciuto come la reincarn...\n",
              "3  https://wordlift.io/blog/it/vocabolario/robert...  Organizzatore del #WMT2017, ha anche portato u...\n",
              "4  https://wordlift.io/blog/it/vocabolario/wordlift/  WordLift è una startup innovativa romana che h..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpapAYpp8Djw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "92fc3732-e2ea-4e40-cdd2-c00d6ae88412"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "# We set the variable forthe name of the CSV where we will store the new MDs \n",
        "outputcsv = 'new-md.csv'\n",
        "print(\"output csv name: \", outputcsv)\n",
        "\n",
        "df_new.to_csv(outputcsv, encoding='utf-8', index=False)\n",
        "print(\"Saving results on:\", outputcsv)\n",
        "files.download(outputcsv)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output csv name:  new-md.csv\n",
            "Saving results on: new-md.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqpldnPS-Q_0"
      },
      "source": [
        "# License\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Andrea Volpini, WordLift\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
      ]
    }
  ]
}